import os, sys, h5py, pickle, numpy as np, logging, os.path as osp
from torch.utils.data import Dataset
from torchvision.transforms import transforms

from data_utils.transforms import *
from pointnet2_ops.pointnet2_utils import *


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(BASE_DIR)


class ScanObjectNNHardest(Dataset):
    """The hardest variant of ScanObjectNN.
    The data we use is: `training_objectdataset_augmentedrot_scale75.h5`[1],
    where there are 2048 points in training and testing.
    The number of training samples is: 11416, and the number of testing samples is 2882.
    Args:
    """
    classes = [
        "bag",
        "bin",
        "box",
        "cabinet",
        "chair",
        "desk",
        "display",
        "door",
        "shelf",
        "table",
        "bed",
        "pillow",
        "sink",
        "sofa",
        "toilet",
    ]
    num_classes = 15
    gravity_dim = 1

    def __init__(self, data_dir, split,
                 num_points=2048,
                 uniform_sample=True,
                 transform=None,
                 **kwargs):
        super().__init__()
        self.partition = split
        self.transform = transform
        self.num_points = num_points
        slit_name = 'training' if split == 'train' else 'test'
        h5_name = os.path.join(
            data_dir, f'{slit_name}_objectdataset_augmentedrot_scale75.h5')

        if not osp.isfile(h5_name):
            raise FileExistsError(
                f'{h5_name} does not exist, please download dataset at first')
        with h5py.File(h5_name, 'r') as f:
            self.points = np.array(f['data']).astype(np.float32)
            self.labels = np.array(f['label']).astype(int)


        if slit_name == 'test' and uniform_sample:
            precomputed_path = os.path.join(
                data_dir, f'{slit_name}_objectdataset_augmentedrot_scale75_1024_fps.pkl')

            if not os.path.exists(precomputed_path):

                points = torch.from_numpy(self.points).to(torch.float32).cuda()
                self.points = fps(points, 1024).cpu().numpy()
                with open(precomputed_path, 'wb') as f:
                    pickle.dump(self.points, f)
                    print(f"{precomputed_path} saved successfully")
            else:
                with open(precomputed_path, 'rb') as f:
                    self.points = pickle.load(f)
                    print(f"{precomputed_path} load successfully")
        else:
            precomputed_path = os.path.join(
                data_dir, f'{slit_name}_objectdataset_augmentedrot_scale75_{self.num_points}_fps.pkl')

            if not os.path.exists(precomputed_path):

                points = torch.from_numpy(self.points).to(torch.float32).cuda()
                self.points = fps(points, self.num_points).cpu().numpy()
                with open(precomputed_path, 'wb') as f:
                    pickle.dump(self.points, f)
                    print(f"{precomputed_path} saved successfully")
            else:
                with open(precomputed_path, 'rb') as f:
                    self.points = pickle.load(f)
                    print(f"{precomputed_path} load successfully")


    @property
    def num_classes(self):
        return self.labels.max() + 1

    def __getitem__(self, idx):
        # print(self.points.shape)
        current_points = self.points[idx][:self.num_points]
        label = self.labels[idx]
        if self.partition == 'train':
            np.random.shuffle(current_points)
        data = {'pos': current_points,
                'y': label
                }
        if self.transform is not None:
            data = self.transform(data)

        #height appending. @KPConv
        # TODO: remove pos here, and only use heights.
        if 'heights' in data.keys():
            data['x'] = torch.cat((data['pos'], data['heights']), dim=1)
        else:
            data['x'] = torch.cat((data['pos'],
                                   torch.from_numpy(current_points[:, self.gravity_dim:self.gravity_dim+1] - current_points[:, self.gravity_dim:self.gravity_dim+1].min())), dim=1)
        return data

    def __len__(self):
        return self.points.shape[0]



class ScanObjectNN(Dataset):
    """The hardest variant of ScanObjectNN.
    The data we use is: `training_objectdataset_augmentedrot_scale75.h5`[1],
    where there are 2048 points in training and testing.
    The number of training samples is: 11416, and the number of testing samples is 2882.
    Args:
    """
    classes = [
        "bag",
        "bin",
        "box",
        "cabinet",
        "chair",
        "desk",
        "display",
        "door",
        "shelf",
        "table",
        "bed",
        "pillow",
        "sink",
        "sofa",
        "toilet",
    ]
    num_classes = 15
    gravity_dim = 1

    def __init__(self, data_dir, split,
                 num_points=2048,
                 uniform_sample=True,
                 transform=None,
                 **kwargs):
        super().__init__()
        self.partition = split
        self.transform = transform
        self.num_points = num_points
        slit_name = 'training' if split == 'train' else 'test'
        h5_name = os.path.join(
            data_dir, f'{slit_name}_objectdataset.h5')

        if not osp.isfile(h5_name):
            raise FileExistsError(
                f'{h5_name} does not exist, please download dataset at first')
        with h5py.File(h5_name, 'r') as f:
            self.points = np.array(f['data']).astype(np.float32)
            self.labels = np.array(f['label']).astype(int)


        if slit_name == 'test' and uniform_sample:
            precomputed_path = os.path.join(
                data_dir, f'{slit_name}_objectdataset_1024_fps.pkl')

            if not os.path.exists(precomputed_path):

                points = torch.from_numpy(self.points).to(torch.float32).cuda()
                self.points = fps(points, 1024).cpu().numpy()
                with open(precomputed_path, 'wb') as f:
                    pickle.dump(self.points, f)
                    print(f"{precomputed_path} saved successfully")
            else:
                with open(precomputed_path, 'rb') as f:
                    self.points = pickle.load(f)
                    print(f"{precomputed_path} load successfully")
        else:
            precomputed_path = os.path.join(
                data_dir, f'{slit_name}_objectdataset_{self.num_points}_fps.pkl')

            if not os.path.exists(precomputed_path):

                points = torch.from_numpy(self.points).to(torch.float32).cuda()
                self.points = fps(points, self.num_points).cpu().numpy()
                with open(precomputed_path, 'wb') as f:
                    pickle.dump(self.points, f)
                    print(f"{precomputed_path} saved successfully")
            else:
                with open(precomputed_path, 'rb') as f:
                    self.points = pickle.load(f)
                    print(f"{precomputed_path} load successfully")


    @property
    def num_classes(self):
        return self.labels.max() + 1

    def __getitem__(self, idx):
        # print(self.points.shape)
        current_points = self.points[idx][:self.num_points]
        label = self.labels[idx]
        if self.partition == 'train':
            np.random.shuffle(current_points)
        data = {'pos': current_points,
                'y': label
                }
        if self.transform is not None:
            data = self.transform(data)

        #height appending. @KPConv
        # TODO: remove pos here, and only use heights.
        if 'heights' in data.keys():
            data['x'] = torch.cat((data['pos'], data['heights']), dim=1)
        else:
            data['x'] = torch.cat((data['pos'],
                                   torch.from_numpy(current_points[:, self.gravity_dim:self.gravity_dim+1] - current_points[:, self.gravity_dim:self.gravity_dim+1].min())), dim=1)
        return data

    def __len__(self):
        return self.points.shape[0]

def fps(data, number):
    '''
        data B N C
        number int
    '''
    fps_idx = furthest_point_sample(data[:, :, :3].contiguous(), number)
    fps_data = torch.gather(
        data, 1, fps_idx.unsqueeze(-1).long().expand(-1, -1, data.shape[-1]))
    return fps_data

if __name__ == '__main__':

    data_dir = '/home/ubuntu/h5_files/main_split/'
    train_transforms = transforms.Compose([
        PointsToTensor(),
        PointCloudScaling(scale=[0.9, 1.1]),
        PointCloudCenterAndNormalize(gravity_dim=1),
        PointCloudRotation(angle=[0.0,1.0,0.0]),
        # ChromaticDropGPU()
    ])
    val_transforms = transforms.Compose([
        PointsToTensor(),
        PointCloudCenterAndNormalize(gravity_dim=1)
    ])
    TRAIN_DATASET = ScanObjectNN(num_points=2048, split='train', transform=train_transforms,data_dir=data_dir)
    train_loader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=32, shuffle=True,
                                                  num_workers=6, drop_last=True)
    TEST_DATASET = ScanObjectNN(num_points=1024, split='test', transform=val_transforms,data_dir=data_dir)
    test_loader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=32, shuffle=False,
                                                 num_workers=6)

    for batch_id, data in enumerate(train_loader, 0):
        print(batch_id)
        print(data['x'].shape)
        print(data['y'].shape)

    print(f"length of training dataset: {len(train_loader.dataset)}")
    print(f"length of validation dataset: {len(test_loader.dataset)}")
